<?xml version="1.0" encoding="UTF-8" ?>
<record xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:tib.eu:TIBKAT:394337816</identifier><datestamp>2024-06-13</datestamp><setSpec>TIBKAT</setSpec></header><metadata>

<documentContainer xsi:schemaLocation="http://www.tib-hannover.de/ext/schema/2007-06-26/fiz-tib-schema.xsd http://www.tib-hannover.de/ext/schema/2007-06-26/fiz-tib-schema.xsd">
    <document id="TIBKAT:394337816">
        <systemInfo>
            <supplier shortname="tib">Technische Informationsbibliothek (TIB)</supplier>
            <databaseDate>2004-09-20</databaseDate>
            <changeDate>2024-03-29</changeDate>
            <ftxCreationDate>2024-06-13</ftxCreationDate>
            <documentID>394337816</documentID>
            <exportRestricted>false</exportRestricted>
            <searchSpaces mappingCriterionFIDBAU="datapool" mappingCriterionFIDMOVE="datapool" mappingCriterionOER="datapool" mappingCriterionTIBUB="datapool">
                <searchSpace>TIBUB</searchSpace>
            </searchSpaces>
            <licenseModels>
                <licenseModel>com</licenseModel>
            </licenseModels>
        </systemInfo>
        <formalInfo>
            <documentTypes>
                <documentAdvancedType>
                    <documentGenreGroup>
                        <documentGenre>
                            <documentGenreCode>D</documentGenreCode>
                        </documentGenre>
                    </documentGenreGroup>
                    <documentTypeGroup>
                        <documentType>
                            <documentTypeCode>PR</documentTypeCode>
                        </documentType>
                    </documentTypeGroup>
                </documentAdvancedType>
            </documentTypes>
            <documentLanguages>
                <documentLanguage>
                    <languageCodes>
                        <code iso="639-1">de</code>
                        <code iso="639-2">ger</code>
                    </languageCodes>
                </documentLanguage>
            </documentLanguages>
            <identifiers>
                <identifier type="ppn">394337816</identifier>
                <identifier type="firstid">GBV:394337816</identifier>
            </identifiers>
            <localHoldings>
                <localHolding database="TIBKAT">
                    <identifier type="epn">731452763</identifier>
                    <creationDate>2005-08-05</creationDate>
                    <selectionKey>z</selectionKey>
                    <location type="tibLocationCode">Haus2</location>
                    <location type="tibShelfmark">H 05 B 1562</location>
                    <loanIndicator>u</loanIndicator>
                    <licenseModels>
                        <licenseModel>com</licenseModel>
                    </licenseModels>
                </localHolding>
            </localHoldings>
            <sizes>
                <size unit="unknown">XVI, 150 S</size>
            </sizes>
        </formalInfo>
        <bibliographicInfo dependent="false">
            <title>Heterogene Sensordatenfusion zur robusten Objektverfolgung im automobilen Straßenverkehr</title>
            <abstracts>
                <abstract xml:lang="de">Zukünftige Fahrerassistenzsysteme benötigen neben der Information über den aktuellen Fahrzustand eine Umgebungserfassung und -modellierung. Je nach Komplexität der Assistenzfunktion wird diese einen unterschiedlichen Detaillierungsgrad aufweisen, wobei für Sicherheitssysteme - wie beispielsweise einen automatischen Bremseingriff - die höchsten Anforderungen gelten. Sowohl die geometrischen als auch die dynamischen Merkmale potenzieller Kollisionspartner müssen möglichst genau bestimmt werden. Da keine heute verfügbare Sensorik in der Lage ist, allen Anforderungen gerecht zu werden, ist die Sensordatenfusion von heterogenen Sensoren erforderlich. Hierbei können die Vorteile von Radarsensoren bei der Bestimmung der dynamischen Objekteigenschaften mit den Vorteilen optischer Systeme bei der Bestimmung der geometrischen Merkmale kombiniert werden. Für eine optimale Fusion sind jedoch unterschiedliche Aspekte zu berücksichtigen, von denen im Anschluss an einen umfassenden Literaturüberblick drei behandelt werden: Die Verarbeitung asynchroner Sensordaten, ein verbessertes Sensormodell insbesondere für optische Sensoren und eine Fusionsarchitektur für variable Beobachtbarkeit von Zustandsgrößen. Im Rahmen der Arbeit wird auf konkrete Modifikationsmöglichkeiten einer Kalman-Filter-Architektur eingegangen, um die Objektverfolgung insbesondere beim Wechsel der Perspektive - wie sie beispielsweise bei Überholmanövern auftritt - zu stabilisieren. Die Vorteile der vorgestellten Algorithmen werden anhand von realen Messdaten demonstriert. &lt;dt.&gt;</abstract>
                <abstract xml:lang="en">Future driver assistance systems will require not only information about the current state of a vehicle but also a perception and modelling of the environment. Depending on the complexity of the assistance function it will have a variable scale of details. Safety systems - like automatic breaking - have to meet the highest requirements. The geometric as well as the dynamic parameters of potential collision partners have to be calculated as precisely as possible. Because no state-of-the-art sensor device is able to meet the whole variety of requirements, a sensor data fusion of heterogenous sensors is therefore essential. Here the advantages of radar sensors measuring the dynamic properties of objects can be combined with the advantages of optical sensors measuring their geometric dimension. In order to achieve an optimal fusion, however, different aspects have to be considered. Three of those are covered by this thesis and will be presented after a detailed bibliography: The processing of asynchronous sensor data, an improved sensor model for optical sensors in particular, and finally a fusion architecture for a varying observability of the state vector. This thesis will show different possibilities to modify a Kalman filter architecture in order to stabilize object tracking especially when the perspective is changing - which is typical for overtaking manoeuvres. The advantages of the proposed algorithms will be demonstrated on the basis of real sensor data. &lt;engl.&gt;</abstract>
            </abstracts>
            <contributors>
                <contributor type="author">
                    <type>author</type>
                    <contributor>Stüker, Dirk</contributor>
                </contributor>
            </contributors>
            <responsible>von Dirk Stüker</responsible>
            <publicationInfo>
                <issued>2003</issued>
                <publicationCountries>
                    <publicationCountry code="DE" />
                </publicationCountries>
            </publicationInfo>
            <additionalDocumentInfo>
                <dissertationInfos>
                    <dissertationInfo>
                        <description>Oldenburg, Univ., Diss., 2004</description>
                    </dissertationInfo>
                </dissertationInfos>
                <publicationIDInfo>Auch als elektronisches Dokument vorh.</publicationIDInfo>
            </additionalDocumentInfo>
            <description>Ill., graph. Darst</description>
        </bibliographicInfo>
        <classificationInfo>
            <classifications>
                <classification classificationID="106416383" classificationName="bk">
                    <code>55.20</code>
                    <entries>
                        <entry>Straßenfahrzeugtechnik</entry>
                    </entries>
                </classification>
                <classification classificationID="10641030X" classificationName="bk">
                    <code>54.74</code>
                    <entries>
                        <entry>Maschinelles Sehen</entry>
                    </entries>
                </classification>
                <classification classificationName="linsearch" classificationProcedure="mapping">
                    <code>ver</code>
                </classification>
                <classification classificationName="linsearch" classificationProcedure="mapping">
                    <code>inf</code>
                </classification>
            </classifications>
            <subjects>
                <subject id="4311226-2" type="gnd">
                    <subject xml:lang="de">Objektverfolgung</subject>
                </subject>
                <subject id="4307964-7" type="gnd">
                    <subject xml:lang="de">Sensorsystem</subject>
                </subject>
                <subject id="4809985-5" type="gnd">
                    <subject xml:lang="de">Radarsensor</subject>
                </subject>
                <subject id="4582612-2" type="gnd">
                    <subject xml:lang="de">Datenfusion</subject>
                </subject>
                <subject id="4075677-4" type="gnd">
                    <subject xml:lang="de">Optischer Sensor</subject>
                </subject>
                <subject id="4622983-8" type="gnd">
                    <subject xml:lang="de">Fahrerassistenzsystem</subject>
                </subject>
            </subjects>
        </classificationInfo>
        <displayInfo>
            <property name="displayDownload">none</property>
            <property name="displayOrder">true</property>
            <property name="displayGetItem">true</property>
        </displayInfo>
    </document>
</documentContainer>
</metadata></record>